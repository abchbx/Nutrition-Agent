# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import Any, Type

import numpy as np
from faiss import IndexFlatL2
from langchain_community.docstore import InMemoryDocstore
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pydantic import BaseModel, Field

from config import AGENT_MODEL, AGENT_TEMPERATURE

# Setup logger for nutrition_qa_tool.py
from config import agent_logger as logger  # Re-use agent logger or create a new one if needed


class NutritionQAInput(BaseModel):
    """è¥å…»å­¦é—®ç­”å·¥å…·è¾“å…¥æ¨¡å‹"""

    question: str = Field(description="è¥å…»å­¦ç›¸å…³é—®é¢˜")
    detail_level: str = Field(default="ä¸­ç­‰", description="å›ç­”è¯¦ç»†ç¨‹åº¦ï¼šç®€å•ã€ä¸­ç­‰ã€è¯¦ç»†")


class NutritionQATool(BaseTool):
    """è¥å…»å­¦çŸ¥è¯†é—®ç­”å·¥å…·"""

    name: str = "nutrition_qa"
    description: str = "å›ç­”è¥å…»å­¦ç›¸å…³çš„çŸ¥è¯†é—®é¢˜ï¼ŒåŒ…æ‹¬è¥å…»åŸç†ã€å¥åº·é¥®é£Ÿã€è¥å…»ç´ åŠŸèƒ½ç­‰"
    args_schema: Type[BaseModel] = NutritionQAInput

    llm: Any = None
    embeddings: Any = None
    knowledge_base: Any = None
    qa_prompt: Any = None
    qa_chain: Any = None

    def __init__(self, embeddings: HuggingFaceEmbeddings):
        super().__init__()
        self.llm = ChatOpenAI(model_name=AGENT_MODEL, temperature=AGENT_TEMPERATURE)
        self.embeddings = embeddings
        self.knowledge_base = self._create_knowledge_base()

        self.qa_prompt = PromptTemplate(
            input_variables=["question", "context", "detail_level"],
            template="""
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æ³¨å†Œè¥å…»å¸ˆ (RDN)ï¼Œåå«"å°è¥"ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œç»“æ„åŒ–åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**ç”¨æˆ·é—®é¢˜:** {question}

**ç›¸å…³è¥å…»å­¦çŸ¥è¯†:**
{context}

**å›ç­”è¯¦ç»†ç¨‹åº¦è¦æ±‚:** {detail_level}

---
**## è¾“å‡ºæ ¼å¼è¦æ±‚**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ Markdown æ ¼å¼ç»„ç»‡ä½ çš„å›ç­”ï¼Œç¡®ä¿å†…å®¹å±‚æ¬¡åˆ†æ˜ï¼Œå¹¶ä¸”å¯ä»¥ç›´æ¥è¢«ä¸» Agent ç”¨ä½œ `nutrition_qa` æ¨¡æ¿çš„å†…å®¹ï¼š

#### ğŸ¯ æ ¸å¿ƒç­”æ¡ˆ
* (ç”¨ä¸€ä¸¤å¥è¯ç›´æ¥å›ç­”é—®é¢˜çš„æ ¸å¿ƒ)

#### ğŸ”¬ è¯¦ç»†è§£é‡Š
* (æ ¹æ®è¯¦ç»†ç¨‹åº¦è¦æ±‚ï¼Œå±•å¼€è§£é‡ŠèƒŒåçš„ç§‘å­¦åŸç†å’Œé€»è¾‘)
* (å¯ä»¥åˆ—å‡ºå¤šä¸ªè¦ç‚¹ï¼Œå¹¶å¯¹ **å…³é”®è¯** è¿›è¡ŒåŠ ç²—)

#### ğŸ‘ å®è·µå»ºè®®
* (æä¾›2-3æ¡å…·ä½“ã€å¯æ“ä½œçš„ç”Ÿæ´»æˆ–é¥®é£Ÿå»ºè®®)

è¯·ç¡®ä¿å›ç­”ç§‘å­¦å‡†ç¡®ã€è¯­è¨€é€šä¿—æ˜“æ‡‚ã€‚
åˆç†ä½¿ç”¨è¡¨æƒ…ç¬¦å·å’Œæ ¼å¼åŒ–æ¥å¢å¼ºå¯è¯»æ€§ã€‚
é‡è¦ä¿¡æ¯è¦çªå‡ºæ˜¾ç¤ºï¼Œå¤æ‚æ¦‚å¿µè¦ç”¨ç®€å•è¯­è¨€è§£é‡Šã€‚
""",
        )
        self.qa_chain = self.qa_prompt | self.llm

    def _create_knowledge_base(self) -> FAISS:
        # å°è¯•ä»å¤–éƒ¨ Markdown æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“
        knowledge_base_path = "knowledge_base.md"
        nutrition_docs = []

        if os.path.exists(knowledge_base_path):
            try:
                with open(knowledge_base_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # ç®€å•åœ°å°†æ•´ä¸ªæ–‡ä»¶å†…å®¹ä½œä¸ºä¸€ä¸ªæ–‡æ¡£å¤„ç†ï¼Œæˆ–å¯ä»¥æŒ‰ç« èŠ‚åˆ†å‰²
                # è¿™é‡Œæˆ‘ä»¬æŒ‰ '####' æ ‡é¢˜åˆ†å‰²æˆå¤šä¸ªæ–‡æ¡£
                sections = content.split("\n#### ")
                for i, section in enumerate(sections):
                    if section.strip():
                        # é‡æ–°åŠ ä¸Šæ ‡é¢˜å‰ç¼€
                        page_content = section if i == 0 else "#### " + section
                        nutrition_docs.append(
                            Document(
                                page_content=page_content.strip(),
                                metadata={"source": "knowledge_base.md", "section_id": i},
                            )
                        )
                print(f"âœ… ä» {knowledge_base_path} åŠ è½½äº† {len(nutrition_docs)} ä¸ªçŸ¥è¯†ç‰‡æ®µã€‚")
            except Exception as e:
                print(f"âš ï¸ ä» {knowledge_base_path} åŠ è½½çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
        else:
            print(f"âš ï¸ çŸ¥è¯†åº“æ–‡ä»¶ {knowledge_base_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤çŸ¥è¯†ã€‚")

        # å¦‚æœæ²¡æœ‰ä»æ–‡ä»¶åŠ è½½åˆ°å†…å®¹ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„ç¡¬ç¼–ç çŸ¥è¯†
        if not nutrition_docs:
            nutrition_docs = [
                Document(
                    page_content="å®é‡è¥å…»ç´ åŒ…æ‹¬ç¢³æ°´åŒ–åˆç‰©ã€è›‹ç™½è´¨å’Œè„‚è‚ªï¼Œæ˜¯èº«ä½“èƒ½é‡çš„ä¸»è¦æ¥æºã€‚",
                    metadata={"category": "å®é‡è¥å…»ç´ ", "topic": "åŸºç¡€"},
                ),
                Document(
                    page_content="è›‹ç™½è´¨æ˜¯æ„æˆè‚Œè‚‰ã€å™¨å®˜å’Œé…¶çš„åŸºç¡€ï¼Œå¯¹äºç”Ÿé•¿å’Œä¿®å¤è‡³å…³é‡è¦ã€‚",
                    metadata={"category": "å®é‡è¥å…»ç´ ", "topic": "è›‹ç™½è´¨"},
                ),
                Document(
                    page_content="è†³é£Ÿçº¤ç»´æœ‰åŠ©äºè‚ é“å¥åº·ï¼Œèƒ½å¢åŠ é¥±è…¹æ„Ÿï¼Œå¸¸è§äºè”¬èœã€æ°´æœå’Œå…¨è°·ç‰©ä¸­ã€‚",
                    metadata={"category": "å…¶ä»–è¥å…»ç´ ", "topic": "è†³é£Ÿçº¤ç»´"},
                ),
            ]

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        texts = text_splitter.split_documents(nutrition_docs)

        try:
            print("ğŸ’¡ æ­£åœ¨ä¸ºQAå·¥å…·åˆ›å»ºçŸ¥è¯†åº“ (ä½¿ç”¨ IndexFlatL2)...")

            embeddings_vectors = self.embeddings.embed_documents([doc.page_content for doc in texts])

            # æ£€æŸ¥æ˜¯å¦æœ‰åµŒå…¥å‘é‡ç”Ÿæˆ
            if not embeddings_vectors:
                print("âš ï¸ æœªèƒ½ç”ŸæˆåµŒå…¥å‘é‡ï¼Œä½¿ç”¨ FAISS çš„ from_documents æ–¹æ³•åˆ›å»ºçŸ¥è¯†åº“")
                return FAISS.from_documents(texts, self.embeddings)

            vectors = np.array(embeddings_vectors, dtype=np.float32)

            # æ£€æŸ¥å‘é‡ç»´åº¦
            if vectors.size == 0 or len(vectors.shape) < 2:
                print("âš ï¸ åµŒå…¥å‘é‡ç»´åº¦ä¸æ­£ç¡®ï¼Œä½¿ç”¨ FAISS çš„ from_documents æ–¹æ³•åˆ›å»ºçŸ¥è¯†åº“")
                return FAISS.from_documents(texts, self.embeddings)

            embedding_dimension = vectors.shape[1]

            index = IndexFlatL2(embedding_dimension)
            index.add(vectors)

            docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(texts)})
            index_to_docstore_id = {i: str(i) for i in range(len(texts))}

            knowledge_base = FAISS(
                embedding_function=self.embeddings.embed_query,
                index=index,
                docstore=docstore,
                index_to_docstore_id=index_to_docstore_id,
            )

            print("âœ… QAå·¥å…·çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ!")
            return knowledge_base

        except Exception as e:
            print(f"âŒ åˆ›å»ºQAçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            return FAISS.from_documents(texts, self.embeddings)

    def _run(self, question: str, detail_level: str = "ä¸­ç­‰") -> str:
        try:
            relevant_docs = self.knowledge_base.similarity_search(question, k=3)
            context = "\n\n".join(doc.page_content for doc in relevant_docs)
            if not context:
                context = "æœªæ‰¾åˆ°ç›¸å…³çš„è¥å…»å­¦çŸ¥è¯†æ–‡æ¡£ï¼Œå°†åŸºäºä¸“ä¸šçŸ¥è¯†å›ç­”ã€‚"

            response = self.qa_chain.invoke({"question": question, "context": context, "detail_level": detail_level})
            return response.content
        except Exception as e:
            return f"å›ç­”è¥å…»å­¦é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


class NutritionMythInput(BaseModel):
    """è¥å…»è¯¯åŒºè¾¨æå·¥å…·è¾“å…¥æ¨¡å‹"""

    myth: str = Field(description="éœ€è¦è¾¨æçš„è¥å…»è¯¯åŒºæˆ–æµè¡Œè¯´æ³•")


class NutritionMythTool(BaseTool):
    """è¥å…»è¯¯åŒºè¾¨æå·¥å…·"""

    name: str = "nutrition_myth"
    description: str = "è¾¨æè¥å…»å­¦è¯¯åŒºå’Œæµè¡Œè¯´æ³•ï¼Œæä¾›ç§‘å­¦ä¾æ®"
    args_schema: Type[BaseModel] = NutritionMythInput

    llm: Any = None
    myth_prompt: Any = None
    myth_chain: Any = None

    def __init__(self):
        super().__init__()
        self.llm = ChatOpenAI(model_name=AGENT_MODEL, temperature=AGENT_TEMPERATURE)

        self.myth_prompt = PromptTemplate(
            input_variables=["myth"],
            template="""
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æ³¨å†Œè¥å…»å¸ˆ (RDN)ï¼Œåå«"å°è¥"ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç§‘å­¦åœ°è¾¨æä»¥ä¸‹è¥å…»è¯¯åŒºæˆ–æµè¡Œè¯´æ³•ã€‚

**## ğŸ§ å¾…è¾¨æè¯´æ³•**
{myth}

---
**## è¾“å‡ºæ ¼å¼è¦æ±‚**
è¯·ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¿›è¡Œåˆ†æï¼Œæ¯ä¸ªéƒ¨åˆ†ä½¿ç”¨äºŒçº§æ ‡é¢˜ã€‚è¿™éƒ¨åˆ†çš„è¾“å‡ºå¯ä»¥ç›´æ¥è¢«ä¸» Agent ç”¨ä½œ `nutrition_qa` æ¨¡æ¿çš„å†…å®¹ï¼š

#### ğŸ¯ æ ¸å¿ƒç­”æ¡ˆ
* (ç›´æ¥ç‚¹æ˜è¯¥è¯´æ³•æ˜¯æ­£ç¡®ã€é”™è¯¯è¿˜æ˜¯æœ‰å±€é™æ€§)

#### ğŸ”¬ è¯¦ç»†è§£é‡Š
* (ä»ç§‘å­¦è§’åº¦å‡ºå‘ï¼Œåˆ—å‡ºä¸æ­¤è¯´æ³•ç›¸å…³çš„æ ¸å¿ƒäº‹å®å’Œç ”ç©¶è¯æ®ï¼Œå¹¶å°† **å…³é”®è¯** åŠ ç²—)
* (åˆ†æè¯¥è¯´æ³•å¯èƒ½çš„èµ·æºå’Œæµè¡Œçš„åŸå› )

#### ğŸ‘ å®è·µå»ºè®®
* (é’ˆå¯¹è¿™ä¸ªè¯¯åŒºï¼Œæä¾›ç§‘å­¦ã€å¯è¡Œçš„é¥®é£Ÿæˆ–ç”Ÿæ´»æ–¹å¼å»ºè®®)
* (å¦‚æœè¯¥è¯´æ³•æœ‰éƒ¨åˆ†æ­£ç¡®æ€§ï¼Œè¯´æ˜åœ¨ä»€ä¹ˆæƒ…å†µä¸‹é€‚ç”¨)

è¯·ç¡®ä¿å›ç­”ç§‘å­¦å‡†ç¡®ã€è¯­è¨€é€šä¿—æ˜“æ‡‚ã€‚
åˆç†ä½¿ç”¨è¡¨æƒ…ç¬¦å·å’Œæ ¼å¼åŒ–æ¥å¢å¼ºå¯è¯»æ€§ã€‚
é‡è¦ä¿¡æ¯è¦çªå‡ºæ˜¾ç¤ºï¼Œå¤æ‚æ¦‚å¿µè¦ç”¨ç®€å•è¯­è¨€è§£é‡Šã€‚
""",
        )
        self.myth_chain = self.myth_prompt | self.llm

    def _run(self, myth: str) -> str:
        try:
            response = self.myth_chain.invoke({"myth": myth})
            return response.content
        except Exception as e:
            return f"è¾¨æè¥å…»è¯¯åŒºæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
